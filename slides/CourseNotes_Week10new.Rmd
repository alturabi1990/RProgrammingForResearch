---
title: "Entering and cleaning data #3"
header-includes:
   - \usepackage{hyperref}
   - \definecolor{links}{HTML}{2A1B81}
   - \hypersetup{colorlinks,linkcolor=,urlcolor=links}
output: 
  beamer_presentation:
    theme: "metropolis"
fontsize: 10pt
---

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
library(dplyr)
library(readr)
```

# Final project 

## Groups for final project 

- Group 1: Chaoyu, Jessica, Erin, Daniel
- Group 2: Heather, Ser$\'{e}$, Khum, Sunny
- Group 3: Rachel, Shayna, Sherry, Matthew
- Group 4: Caroline, Jacob, Eric, Burton
- Group 5: Nikki, Collin, Kyle, Molly

## Proposed time for final presentations

Friday, December 13, 2:00 pm--4:00 pm

# Pulling online data

## APIs

API: "Application Program Interface" \bigskip

An API provides the rules for software applications to interact. In the case of open data APIs, they provide the rules you need to know to write R code to request and pull data from the organization's web server into your R session. \bigskip

Often, an API can help you avoid downloading all available data, and instead only download the subset you need.

## APIs

Strategy for using APIs from R: 

- Figure out the API rules for HTTP requests
- Write R code to create a request in the proper format 
- Send the request using GET or POST HTTP methods
- Once you get back data from the request, parse it into an easier-to-use format if necessary 

## API documentation

Start by reading any documentation available for the API. This will often give information on what data is available and how to put together requests. 

```{r echo = FALSE, out.width = "1.1\\textwidth", fig.align = "center"}
knitr::include_graphics("../figures/NASA_API_documentation.png")
```

Source: https://api.nasa.gov/

## API key

Many organizations will require you to get an API key and use this key in each of your API requests. This key allows the organization to control API access, including enforcing rate limits per user. API rate limits restrict how often you can request data (e.g., an hourly limit of 1,000 requests per user for NASA APIs). \bigskip

You should keep this key private. In particular, make sure you do not include it in code that is posted to GitHub. 

## Example---`riem` package

The `riem` package, developed by Maelle Salmon and an ROpenSci package, is an excellent and straightforward example of how you can use R to pull open data through a web API. \bigskip

This package allows you to pull weather data from airports around the world directly from the [Iowa Environmental Mesonet](https://mesonet.agron.iastate.edu/request/download.phtml).

## Example---`riem` package

To get a certain set of weather data from the Iowa Environmental Mesonet, you can send an HTTP request specifying a base URL, "https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/", as well as some parameters describing the subset of dataset you want (e.g., date ranges, weather variables, output format). \bigskip

Once you know the rules for the names and possible values of these parameters (more on that below), you can submit an HTTP GET request using the `GET` function from the `httr` package. 

## Example---`riem` package

```{r echo = FALSE, fig.align = "center", out.width = "\\textwidth"}
knitr::include_graphics("../figures/mesonet_example.png")
```

https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=DEN&data=sknt&year1=2016&month1=6&day1=1&year2=2016&month2=6&day2=30&tz=America%2FDenver&format=comma&latlon=no&direct=no&report_type=1&report_type=2


## Using `httr` to get data from a webpage

When you are making an HTTP request using the `GET` or `POST` functions from the `httr` package, you can include the key-value pairs for any query parameters as a list object in the `query` argurment of the function.

```{r}
library(httr)
meso_url <- paste0("https://mesonet.agron.iastate.edu/",
                   "cgi-bin/request/asos.py/")
denver <- GET(url = meso_url,
              query = list(station = "DEN", data = "sped",
                           year1 = "2016", month1 = "6",
                           day1 = "1", year2 = "2016",
                           month2 = "6", day2 = "30",
                           tz = "America/Denver",
                           format = "comma"))
```

## Using `httr` to get data from a webpage

The `GET` call will return a special type of list object with elements that include the url you queried and the content of the page at that url:

```{r}
str(denver, max.level = 1, list.len = 6)
```

## Using `httr` to get data from a webpage

The `httr` package includes functions to pull out elements of this list object, including: 

- `headers`: Pull out the header information
- `content`: Pull out the content returned from the page
- `status_code`: Pull out the status code from the `GET` request (e.g., 200: okay; 404: not found)

Note: For some fun examples of 404 pages, see https://www.creativebloq.com/web-design/best-404-pages-812505

## Using `httr` to get data from a webpage

You can use `content` from `httr` to retrieve the contents of the HTTP request we made. For this particular web data, the requested data is a comma-separated file, so you can convert it to a dataframe with `read_csv`:

```{r}
denver %>% content() %>% 
  read_csv(skip = 5, na = "M") %>%
  slice(1:3)
```

## Example---`riem` package

The `riem` package wraps up this whole process, so you can call a single function to get in 
the data you want from the API:

\footnotesize

```{r}
library(riem)
denver_2 <- riem_measures(station = "DEN", 
                          date_start = "2016-06-01",
                          date_end = "2016-06-30")
denver_2 %>% slice(1:3) 
```



# Example R API wrappers

## `tigris` package

- Location boundaries 
    + States
    + Counties
    + Blocks
    + Tracks
    + School districts
    + Congressional districts
- Roads
    + Primary roads
    + Primary and secondary roads
- Water
    + Area-water 
    + Linear-water
    + Coastline
- Other
    + Landmarks
    + Military

## `tigris` package

Example from: Kyle Walker. 2016. "tigris: An R Package to Access and Work
with Geographic Data from the US
Census Bureau". The R Journal.

```{r echo = FALSE, out.width = "0.6\\textwidth", fig.align = "center"}
knitr::include_graphics("../figures/tigris_example.png")
```

## US Census packages

A number of other R packages also help you access and use data from the U.S. Census: 

- `tidycensus`: Interface with the US Census Bureauâ€™s decennial Census and five-year American Community APIs and return tidyverse-ready data frames
- `acs`: Download, manipulate, and present American Community Survey and Decennial data from the US Census (see "Working with the American Community Survey in R: A Guide to Using the
acs Package", a book available free online through the CSU library)
- `USABoundaries`: Historical and contemporary boundaries of the United States of America
- `idbr`: R interface to the US Census Bureau International Data Base API (e.g., populations of other countries)

## rOpenSci

rOpenSci (https://ropensci.org): 

\bigskip

> "At rOpenSci we are creating packages that allow access to data repositories through the R statistical programming environment that is already a familiar part of the workflow of many scientists. Our tools not only facilitate drawing data into an environment where it can readily be manipulated, but also one in which those analyses and methods can be easily shared, replicated, and extended by other researchers."

## rOpenSci

rOpenSci collects a number of packages for tapping into open data for research: https://ropensci.org/packages

Some examples (all descriptions from rOpenSci): 

- `AntWeb`: Access data from the world's largest ant database
- `chromer`: Interact with the chromosome counts database (CCDB)
- `gender`: Encodes gender based on names and dates of birth
- `musemeta`: R Client for Scraping Museum Metadata, including The Metropolitan Museum of Art, the Canadian Science & Technology Museum Corporation, the National Gallery of Art, and the Getty Museum, and more to come.
- `rusda`: Interface to some USDA databases
- `webchem`: Retrieve chemical information from many sources. Currently includes: Chemical Identifier Resolver, ChemSpider, PubChem, and Chemical Translation Service.

## `rnoaa`

> "Access climate data from NOAA, including temperature and precipitation, as well as sea ice cover data, and extreme weather events"

- Buoy data from the National Buoy Data Center
- Historical Observing Metadata Repository (HOMR))--- climate station metadata
- National Climatic Data Center weather station data
- Sea ice data
- International Best Track Archive for Climate Stewardship (IBTrACS)--- tropical cyclone tracking data
- Severe Weather Data Inventory (SWDI)

<!-- ## `countyweather` -->

<!-- The `countyweather` package wraps the `rnoaa` package to let you pull and aggregate weather at the county level in the U.S. For example, you can pull all data from Miami during Hurricane Andrew:  -->

<!-- ```{r echo = FALSE, out.width = "\\textwidth"} -->
<!-- knitr::include_graphics("../figures/countyweather2.png") -->
<!-- ``` -->

<!-- ## `countyweather` -->

<!-- When you pull the data for a county, the package also maps the contributing weather stations: -->

<!-- ```{r echo = FALSE, out.width = "\\textwidth"} -->
<!-- knitr::include_graphics("../figures/countyweather1.png") -->
<!-- ``` -->

## USGS-R Packages

USGS has a very nice collection of R packages that wrap USGS open data APIs: https://owi.usgs.gov/R/

\bigskip

> "USGS-R is a community of support for users of the R scientific programming language. USGS-R resources include R training materials, R tools for the retrieval and analysis of USGS data, and support for a growing group of USGS-R developers. "

## USGS R Packages

USGS R packages include: 

- `dataRetrieval`: Obtain water quality sample data, streamflow data, and metadata directly from either the USGS or EPA
- `EGRET`: Analysis of long-term changes in water quality and streamflow, including the water-quality method Weighted Regressions on Time, Discharge, and Season (WRTDS)
- `laketemps`: Lake temperature data package for Global Lake Temperature Collaboration Project
- `lakeattributes`: Common useful lake attribute data
- `soilmoisturetools`: Tools for soil moisture data retrieval and visualization

## Other R API wrappers

Here are some examples of other R packages that faciliate use of an API for open data:

- `twitteR`: Twitter 
- `Quandl`: Quandl (financial data)
- `RGoogleAnalytics`: Google Analytics
- `WDI`, `wbstats`: World Bank
- `GuardianR`, `rdian`: The Guardian Media Group
- `blsAPI`: Bureau of Labor Statistics
- `rtimes`: New York Times

## R and APIs

Find out more about writing API packages with this vignette for the httr package: https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html. \bigskip

This document includes advice on error handling within R code that accesses data through an open API.

## Group work

We'll take a break now to set up the GitHub repositories for your final group project.

<!-- # Cleaning very messy data -->

<!-- ## Hurricane tracking data -->

<!-- One version of Atlantic basin hurricane tracks is available here: https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2017-050118.txt. The data is not in a classic delimited format:  -->

<!-- ```{r echo = FALSE, fig.align = "center", out.width = "\\textwidth"} -->
<!-- knitr::include_graphics("../figures/hurrtrackformat.png") -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- This data is formatted in the following way:  -->

<!-- - Data for many storms are included in one file.  -->
<!-- - Data for a storm starts with a shorter line, with values for the storm ID, name, and number of observations for the storm. These values are comma separated. -->
<!-- - Observations for each storm are longer lines. There are multiple observations for each storm, where each observation gives values like the location and maximum winds for the storm at that time.  -->

<!-- ## Hurricane tracking data -->

<!-- Strategy for reading in very messy data:  -->

<!-- 1. Read in all lines individually.  -->
<!-- 2. Use regular expressions to split each line into the elements you'd like to use to fill columns.  -->
<!-- 3. Write functions and / or `map` calls to process lines and use the contents to fill a data frame.  -->
<!-- 4. Once you have the data in a data frame, do any remaining cleaning to create a data frame that is easy to use to answer research questions. -->

<!-- ## Hurricane tracking data -->

<!-- Because the data is not nicely formatted, you can't use `read_csv` or similar functions to read it in.  -->

<!-- However, the `read_lines` function from `readr` allows you to read a text file in one line at a time. You can then write code and functions to parse the file one line at a time, to turn it into a dataframe you can use.  -->

<!-- Note: Base R has `readLines`, which is very similar. -->

<!-- ## Hurricane tracking data -->

<!-- The `read_lines` function from `readr` will read in lines from a text file directly, without trying to separate into columns. You can use the `n_max` argument to specify the number of lines to read it. \bigskip -->

<!-- For example, to read in three lines from the hurricane tracking data, you can run:  -->

<!-- ```{r warning = FALSE, message = FALSE} -->
<!-- tracks_url <- paste0("http://www.nhc.noaa.gov/data/hurdat/", -->
<!--                      "hurdat2-1851-2017-050118.txt") -->
<!-- hurr_tracks <- read_lines(tracks_url, n_max = 3) -->
<!-- hurr_tracks -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- The data has been read in as a vector, rather than a dataframe:  -->

<!-- ```{r warning = FALSE, message = FALSE} -->
<!-- class(hurr_tracks) -->
<!-- length(hurr_tracks) -->
<!-- hurr_tracks[1] -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- You can use regular expressions to break each line up. For example, you can use `str_split` from the `stringr` package to break the first line of the hurricane track data into its three separate components:  -->

<!-- ```{r} -->
<!-- library(stringr) -->
<!-- str_split(hurr_tracks[1], pattern = ",") -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- You can use this to create a list where each element of the list has the split-up version of a line of the original data. First, read in all of the data: -->

<!-- ```{r warning = FALSE, message = FALSE} -->
<!-- tracks_url <- paste0("http://www.nhc.noaa.gov/data/hurdat/", -->
<!--                      "hurdat2-1851-2017-050118.txt") -->
<!-- hurr_tracks <- read_lines(tracks_url) -->
<!-- length(hurr_tracks) -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- Next, use `map` with `str_split` to split each line of the data at the commas: -->

<!-- ```{r} -->
<!-- library(purrr) -->
<!-- hurr_tracks <- map(hurr_tracks, str_split, -->
<!--                    pattern = ",", simplify = TRUE) -->
<!-- hurr_tracks[[1]] -->
<!-- hurr_tracks[[2]][1:2] -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- Next, you want to split this list into two lists, one with the shorter "meta-data" lines and one with the longer "observation" lines. You can use `map_int` to create a vector with the length of each line. You will later use this to identify which lines are short or long.  -->

<!-- ```{r} -->
<!-- hurr_lengths <- map_int(hurr_tracks, length) -->
<!-- hurr_lengths[1:17] -->
<!-- unique(hurr_lengths) -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- You can use bracket indexing to split the `hurr_tracks` into two lists: one with the shorter lines that start each observation (`hurr_meta`) and one with the storm observations (`hurr_obs`). Use bracket indexing with the `hurr_lengths` vector you just created to make that split. -->

<!-- ```{r} -->
<!-- hurr_meta <- hurr_tracks[hurr_lengths == 4] -->
<!-- hurr_obs <- hurr_tracks[hurr_lengths == 21] -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- ```{r} -->
<!-- hurr_meta[1:3] -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- ```{r} -->
<!-- hurr_obs[1:2] -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- Now, you can use `bind_rows` from `dplyr` to change the list of metadata into a dataframe. (You first need to use `as_tibble` with `map` to convert all elements of the list from matrices to dataframes.) -->

<!-- ```{r message = FALSE} -->
<!-- library(dplyr); library(tibble) -->
<!-- hurr_meta <- hurr_meta %>%  -->
<!--   map(as_tibble) %>%  -->
<!--   bind_rows() -->
<!-- hurr_meta %>% slice(1:3) -->
<!-- ``` -->


<!-- ## Hurricane tracking data -->

<!-- You can clean up the data a bit more.  -->

<!-- - First, the fourth column doesn't have any non-missing values, so you can get rid of it: -->

<!-- ```{r} -->
<!-- unique(hurr_meta$V4) -->
<!-- ``` -->

<!-- - Second, the second and third columns include a lot of leading whitespace: -->

<!-- ```{r} -->
<!-- hurr_meta$V2[1:2] -->
<!-- ``` -->

<!-- - Last, we want to name the columns.  -->

<!-- ## Hurricane tracking data -->

<!-- ```{r} -->
<!-- hurr_meta <- hurr_meta %>% -->
<!--   select(-V4) %>% -->
<!--   rename(storm_id = V1, storm_name = V2, n_obs = V3) %>% -->
<!--   mutate(storm_name = str_trim(storm_name), -->
<!--          n_obs = as.numeric(n_obs)) -->
<!-- hurr_meta %>% slice(1:3) -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- Now you can do the same idea with the hurricane observations. First, we'll want to add storm identifiers to that data. The "meta" data includes storm ids and the number of observations per storm. We can take advantage of that to make a `storm_id` vector that will line up with the storm observations.  -->

<!-- ```{r} -->
<!-- storm_id <- rep(hurr_meta$storm_id, times = hurr_meta$n_obs) -->
<!-- head(storm_id, 3) -->
<!-- length(storm_id) -->
<!-- length(hurr_obs) -->
<!-- ``` -->

<!-- ## Hurricane tracking data -->

<!-- ```{r warnings = FALSE, message = FALSE} -->
<!-- hurr_obs <- hurr_obs %>%  -->
<!--   map(as_tibble) %>%  -->
<!--   bind_rows() %>%  -->
<!--   mutate(storm_id = storm_id) -->
<!-- hurr_obs %>% select(V1:V2, V5:V6, storm_id) %>% slice(1:3) -->
<!-- ``` -->

<!-- ## In-course exercise  -->

<!-- We'll take a break now to do the next part of the In-Course Exercise. -->

## Setting up for group project

\footnotesize

Get together with your group for the final project, and you will be setting up your GitHub
repository to use for that project: 

1. Select one person in your group to "host" the repository. 
2. Go through all the steps we took for Homework 5 to create a local
R Project, put it under git version control, and connect it to a remote GitHub 
repository. This only needs to be done for the person "hosting" the repository.
3. Once one person in your group has the GitHub repository you'll all use, that person
should go to GitHub and make the other group members collaborators. Within the 
GitHub repository, go to the "Settings" tab and go to "Collaborators". You should be
able to find and add other GitHub members. Once you do, your other group members
should get email invitations. 

(More on next slide) 

## Setting up for group project

\footnotesize

4. For the other group members, accept your email invitation to collaborate on the 
repository. Now you can "clone" this repository to your own computer, to have a 
local version to work on. Open a command line, use `cd` to change into the 
directory where you'd like to save the R project, and then use: 

```
git clone git@github.com:geanders/ex_repo.git
```

But replace `geanders` with the GitHub handle for your group member who is hosting the
account and `ex_repo` with the repository name for your GitHub repository for the project. 

## Setting up for group project

\footnotesize 

5. Now have one of your group members create an RMarkdown document to use for your 
final project report. That person should create that document on their local version of 
the repository, save their changes, commit them, and then push them to GitHub. 
Check online to make sure it went through.
Everyone else in the group should then "pull" (use the down arrow in your "Git" box
in RStudio) to pull that change to their local version of the repository. 
6. Now explore resolving commit conflicts. Have two members of your group open the 
RMarkdown document and change the "author" input in the YAML of the document to include
their own name. They both should save and commit this change locally. Have one of the 
two push the change to GitHub. The second person should then pull the latest version of the
repo from GitHub. There will be commit conflicts (you'll see a filled box for the file
in the GitHub page). Open that file and look for the section that starts with 
"<<<<". Look at the two versions given for that part of the document, decide what you 
want as the final version, clean up the "<<<", "===", and ">>>" lines, and then commit
and push the changes. 

## Setting up for group project

\footnotesize 

7. Talk with your group members about how you'd like to share the work for the homework. 
Go to the "Issues" page for the GitHub repository and create some Issues for the tasks 
your group will need to do. Try using the `@` notation to reference other people in your
group in the message (e.g., `@geanders` would reference me and send me an email about
the Issue message).
8. Create one practice Issue that's something like "Test closing an issue with commit". 
Then have one group member make some change on their local version of the project, 
commit that change with the message "Close #[x]", where [x] is the number of the test
Issue you opened, and then push the changes. Go online to GitHub and you should see that
that Issue is now "Closed".

